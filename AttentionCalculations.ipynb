{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcRtFWQZ3qzRTpwp3ZB70T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/everestso/Fall24Spring25/blob/main/AttentionCalculations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpuv-g0p2mvg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relationship Between Two Attention Calculations\n",
        "\n",
        "The relationship between the two different forms of attention calculations:\n",
        "\n",
        "1. **\\$ V \\times \\text{softmax}\\left( \\frac{K^T Q}{\\sqrt{D_q}} \\right) \\$**\n",
        "2. **\\$ \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{D_q}} \\right) \\times V \\$**\n",
        "\n",
        "involves both a difference in the calculation of the attention weights and a fundamental shift in how those weights are applied to the values. Let's explore the differences and the impact on the output of these forms.\n",
        "\n",
        "### Standard Self-Attention Formulation: \\$ \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{D_q}} \\right) \\times V \\$\n",
        "In the common and standard approach used in most attention mechanisms (like Transformers), the formula is:\n",
        "\n",
        "$$\n",
        "\\text{Attention} = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{D_q}} \\right) V\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **\\$ Q \\$**: Query matrix of shape (\\$N_{\\text{queries}} \\times D_q\\$).\n",
        "- **\\$ K \\$**: Key matrix of shape (\\$N_{\\text{keys}} \\times D_q\\$).\n",
        "- **\\$ V \\$**: Value matrix of shape (\\$N_{\\text{keys}} \\times D_v\\$).\n",
        "- **\\$ D_q \\$**: Dimensionality of the keys/queries.\n",
        "\n",
        "#### Steps:\n",
        "1. **Similarity Calculation**: \\$ Q K^T \\$ results in an attention score matrix of shape (\\$N_{\\text{queries}} \\times N_{\\text{keys}}\\$), where each element represents the similarity between a query and a key.\n",
        "2. **Scaling**: The division by \\$ \\sqrt{D_q} \\$ helps stabilize gradients.\n",
        "3. **Softmax Application**: Applying softmax row-wise creates a probability distribution over the keys for each query.\n",
        "4. **Weighted Sum of Values**: Multiplying by the value matrix (\\$ V \\$) results in a weighted sum of the values, producing the final output of shape (\\$N_{\\text{queries}} \\times D_v\\$).\n",
        "\n",
        "In this formulation, **each query vector attends to all keys**, and the resulting attention weights are used to create a weighted sum of the corresponding values. This approach emphasizes that **the focus is on each query finding relevant information across the entire sequence of keys and their associated values**.\n",
        "\n",
        "### Alternate Formulation: \\$ V \\times \\text{softmax}\\left( \\frac{K^T Q}{\\sqrt{D_q}} \\right) \\$\n",
        "In this less common form:\n",
        "\n",
        "$$\n",
        "\\text{Attention} = V \\times \\text{softmax}\\left( \\frac{K^T Q}{\\sqrt{D_q}} \\right)\n",
        "$$\n",
        "\n",
        "#### Steps:\n",
        "1. **Similarity Calculation**: \\$ K^T Q \\$ results in a score matrix of shape (\\$N_{\\text{keys}} \\times N_{\\text{queries}}\\$), effectively reversing the order compared to the standard approach.\n",
        "2. **Scaling**: Again, the division by \\$ \\sqrt{D_q} \\$ helps stabilize the gradients.\n",
        "3. **Softmax Application**: Applying softmax column-wise creates a probability distribution over the queries for each key.\n",
        "4. **Value Weighting**: In this case, the value matrix (\\$ V \\$) is left-multiplied by the softmax result, which fundamentally changes the interpretation of how attention is applied.\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "1. **Role of Keys and Queries**:\n",
        "   - In the standard approach (\\$ Q K^T \\$), **queries are used to determine the relevance of keys**, and the attention weights represent how much attention each query should pay to all keys.\n",
        "   - In the alternate form (\\$ K^T Q \\$), **keys are used to determine the relevance of queries**, and the attention weights represent how much each key attends to all queries. This changes the directional meaning of attention.\n",
        "\n",
        "2. **Softmax Application and Dimensionality**:\n",
        "   - In the standard form, the softmax is applied over rows, meaning each query attends over all keys. This aligns with the idea that each input element (represented by a query) should gather information from all other elements (keys).\n",
        "   - In the alternate form, softmax is applied over columns, meaning each key attends over all queries. This changes the emphasis to **how keys distribute their relevance among the queries**, which is not the typical intention of self-attention mechanisms.\n",
        "\n",
        "3. **Impact on the Output**:\n",
        "   - The output in the standard case (\\$ \\text{softmax}(Q K^T) V \\$) has the shape (\\$N_{\\text{queries}} \\times D_v\\$), indicating that each query has produced an output by attending to all values.\n",
        "   - In the alternate case (\\$ V \\text{softmax}(K^T Q) \\$), the output shape also ends up as (\\$N_{\\text{queries}} \\times D_v\\$), but the resulting attention focuses differently â€” it essentially weights the values based on how much each key attends to queries, which is conceptually reversed compared to the standard approach.\n",
        "\n",
        "### Practical Implications\n",
        "- **Standard Approach (\\$ Q K^T \\$)**: The approach aligns with the idea that each token in a sequence queries the rest of the sequence to gather relevant contextual information, which is crucial for tasks like translation and text generation.\n",
        "- **Alternate Approach (\\$ K^T Q \\$)**: This changes the interpretation to having keys attend to queries, which is less intuitive and might not effectively capture the relationships typically required for self-attention tasks. The network might still learn useful representations, but it fundamentally changes how attention is distributed.\n",
        "\n",
        "### Conclusion\n",
        "While both formulations yield a matrix of the same dimensions, they represent fundamentally different ways of applying attention. The standard formulation (\\$ Q K^T \\$) is designed for **queries to gather information from keys**, which makes more sense for tasks that require context gathering or focus from one element across others. The alternate version (\\$ K^T Q \\$) instead makes **keys gather information from queries**, reversing the intended flow of attention and potentially leading to less effective attention distributions for common use cases.\n"
      ],
      "metadata": {
        "id": "XQZcXTyz2nRR"
      }
    }
  ]
}